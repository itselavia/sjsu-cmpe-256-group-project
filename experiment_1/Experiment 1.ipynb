{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import SVDpp\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.model_selection import cross_validate\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(256)\n",
    "np.random.seed(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_data():\n",
    "\n",
    "    with open(\"/Users/akshay/Downloads/australian_users_items 2.json\", \"r\") as raw_input:\n",
    "        with open(\"/Users/akshay/Downloads/australian_users_items_cleaned.json\", \"w\") as cleaned_file:\n",
    "            try:\n",
    "                for line in raw_input:\n",
    "                    withoutSingleQuotes = line.replace('\\'', '\\\"')\n",
    "                    main_item_name_indexes = [m.start() for m in re.finditer('item_name', withoutSingleQuotes)]\n",
    "                    for main_item_name_index in main_item_name_indexes:\n",
    "                        main_item_name_index = main_item_name_index + 13\n",
    "                        current_play_index = withoutSingleQuotes[main_item_name_index:].find(\"playtime_forever\")\n",
    "                        temp = withoutSingleQuotes[main_item_name_index: main_item_name_index + current_play_index - 4 ]\n",
    "                        repeatingDoubleQuotesIndexes = [m.start() for m in re.finditer(\"\\\"\", temp)]\n",
    "                        for i in repeatingDoubleQuotesIndexes:\n",
    "                            toReplaceIndex = main_item_name_index + i\n",
    "                            withoutSingleQuotes = withoutSingleQuotes[:toReplaceIndex] + \"'\" + withoutSingleQuotes[toReplaceIndex + 1:]\n",
    "                    cleaned_file.write(withoutSingleQuotes)\n",
    "            except:\n",
    "                pass\n",
    "if __name__ == \"__main__\":\n",
    "    clean_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import random\n",
    "\n",
    "def create_dataset():\n",
    "    resultRows = []\n",
    "    with open(\"/Users/akshay/Downloads/australian_users_items_cleaned.json\", \"r\") as cleaned_file:\n",
    "        for line in cleaned_file:\n",
    "            try:\n",
    "                json_obj = json.loads(line)\n",
    "                user_id = str(json_obj['user_id'])\n",
    "                items = json_obj['items']\n",
    "                max_playtime = 0\n",
    "                min_playtime = sys.maxsize\n",
    "                for item in items:\n",
    "                    playtime = item['playtime_forever']\n",
    "                    if playtime > 0:\n",
    "                        if playtime > max_playtime:\n",
    "                            max_playtime = playtime\n",
    "                        if playtime < min_playtime:\n",
    "                            min_playtime = playtime\n",
    "                for item in items:\n",
    "                    item_id = str(item['item_id'])\n",
    "                    playtime = item['playtime_forever']\n",
    "                    if playtime > 0:\n",
    "                        #scaled_playtime = round((playtime - min_playtime)/(max_playtime - min_playtime), 10)\n",
    "                        scaled_playtime = (1-(playtime-min_playtime)/(max_playtime-min_playtime)) + 100*((playtime-min_playtime)/(max_playtime-min_playtime))\n",
    "                        resultRows.append([user_id,item_id,scaled_playtime])\n",
    "            except:\n",
    "                pass\n",
    "    random.shuffle(resultRows)\n",
    "    with open(\"/Users/akshay/Downloads/experiment_1_dataset.csv\", \"w\") as experiment_dataset:\n",
    "        experiment_dataset.write(\"user_id,item_id,rating\\n\")\n",
    "        for entry in resultRows:\n",
    "            experiment_dataset.write(entry[0] + \",\" + entry[1] + \",\" + str(entry[2]) + \"\\n\")\n",
    "if __name__ == \"__main__\":\n",
    "    create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv('/Users/akshay/Downloads/experiment_1_dataset_train.csv')\n",
    "reader = Reader(rating_scale=(1, 100))\n",
    "data = Dataset.load_from_df(users[['user_id', 'item_id', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   52.3s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of 108 | elapsed: 16.4min remaining:  2.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.121329780506976\n",
      "{'n_factors': 0, 'n_epochs': 60, 'lr_all': 0.0005, 'reg_all': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed: 19.3min finished\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"n_factors\": [0,1,2],\n",
    "    \"n_epochs\": [10, 50, 60],\n",
    "    \"lr_all\": [0.0005, 0.0007],\n",
    "    \"reg_all\": [0.01, 0.09]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(SVD, param_grid, n_jobs=-1, measures=[\"rmse\"], cv=3, joblib_verbose=10)\n",
    "gs.fit(data)\n",
    "\n",
    "print(gs.best_score[\"rmse\"])\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Processing epoch 20\n",
      "Processing epoch 21\n",
      "Processing epoch 22\n",
      "Processing epoch 23\n",
      "Processing epoch 24\n",
      "Processing epoch 25\n",
      "Processing epoch 26\n",
      "Processing epoch 27\n",
      "Processing epoch 28\n",
      "Processing epoch 29\n",
      "Processing epoch 30\n",
      "Processing epoch 31\n",
      "Processing epoch 32\n",
      "Processing epoch 33\n",
      "Processing epoch 34\n",
      "Processing epoch 35\n",
      "Processing epoch 36\n",
      "Processing epoch 37\n",
      "Processing epoch 38\n",
      "Processing epoch 39\n",
      "Processing epoch 40\n",
      "Processing epoch 41\n",
      "Processing epoch 42\n",
      "Processing epoch 43\n",
      "Processing epoch 44\n",
      "Processing epoch 45\n",
      "Processing epoch 46\n",
      "Processing epoch 47\n",
      "Processing epoch 48\n",
      "Processing epoch 49\n",
      "Processing epoch 50\n",
      "Processing epoch 51\n",
      "Processing epoch 52\n",
      "Processing epoch 53\n",
      "Processing epoch 54\n",
      "Processing epoch 55\n",
      "Processing epoch 56\n",
      "Processing epoch 57\n",
      "Processing epoch 58\n",
      "Processing epoch 59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x10f028730>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd2 = SVD(n_factors=0, n_epochs=60, lr_all=0.0005, reg_all=0.09, verbose=True)\n",
    "svd2.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 15.1563\n",
      "15.156332979086843\n"
     ]
    }
   ],
   "source": [
    "predictions = svd2.test(testset)\n",
    "print(accuracy.rmse(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.55933242841777\n"
     ]
    }
   ],
   "source": [
    "user_id = \"76561198107409283\"\n",
    "item_id = 730\n",
    "pred = svd2.predict(user_id, item_id)\n",
    "print(pred.est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive:  4280\n",
      "True Negative:  620834\n",
      "False Positive:  4321\n",
      "False Negative:  20985\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "resultRows = []\n",
    "with open('/Users/akshay/Downloads/experiment_1_dataset_test_with_class.csv') as csvfile:\n",
    "    fail = 0\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        try:\n",
    "            user_id = row[0]\n",
    "            item_id = int(row[1])\n",
    "            actual_class = row[2]\n",
    "            if actual_class == \"False\":\n",
    "                actual_class = False\n",
    "            else:\n",
    "                actual_class = True\n",
    "            pred = svd2.predict(user_id, item_id)\n",
    "            predicted_rating = float(pred.est)\n",
    "            if predicted_rating > 50.0:\n",
    "                predicted_class = True\n",
    "            else:\n",
    "                predicted_class = False\n",
    "            \n",
    "            if predicted_class == True and actual_class == True:\n",
    "                #print(user_id, item_id, predicted_rating, predicted_class, actual_class)\n",
    "                true_positive = true_positive + 1\n",
    "            elif predicted_class == False and actual_class == False:\n",
    "                true_negative = true_negative + 1\n",
    "            elif predicted_class == True and actual_class == False:\n",
    "                false_positive = false_positive + 1\n",
    "            elif predicted_class == False and actual_class == True:\n",
    "                false_negative = false_negative + 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "       \n",
    "print(\"True Positive: \", true_positive)\n",
    "print(\"True Negative: \", true_negative)\n",
    "print(\"False Positive: \", false_positive)\n",
    "print(\"False Negative: \", false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
