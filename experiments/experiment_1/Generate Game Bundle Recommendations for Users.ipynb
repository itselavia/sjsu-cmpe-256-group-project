{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Game Bundle Recommendations for Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import SVDpp\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.model_selection import cross_validate\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(256)\n",
    "np.random.seed(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Clean the dataset\n",
    "\n",
    "The raw dataset 'australian_users_items.json' is not in valid JSON format. Following steps are required to clean the raw file:\n",
    "- For each line:\n",
    "  - replace the single quotes with double quotes\n",
    "  - After the first step, the field \"item_name\" has double quotes which should be single quote. So to tackle this, find those indexes and replace the double quotes with single quotes\n",
    "  \n",
    "- Example raw line:\n",
    "  - {'user_id': 'evcentric', 'items_count': 137, 'steam_id': '76561198007712555', 'user_url': 'http://steamcommunity.com/id/evcentric', 'items': [{'item_id': '454060', 'item_name': 'Blueprint\"s Tycoon', 'playtime_forever': 23, 'playtime_2weeks': 0}, {'item_id': '466170', 'item_name': 'Idling to Rule the Gods', 'playtime_forever': 28545, 'playtime_2weeks': 1554}]}\n",
    "\n",
    "- Example cleaned line:\n",
    "  - {\"user_id\": \"evcentric\", \"items_count\": 137, \"steam_id\": \"76561198007712555\", \"user_url\": \"http://steamcommunity.com/id/evcentric\", \"items\": [{\"item_id\": \"454060\", \"item_name\": 'Blueprint's Tycoon\", \"playtime_forever\": 23, \"playtime_2weeks\": 0}, {\"item_id\": \"466170\", \"item_name\": \"Idling to Rule the Gods\", \"playtime_forever\": 28545, \"playtime_2weeks\": 1554}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_data():\n",
    "\n",
    "    with open(\"/Users/akshay/Downloads/australian_users_items.json\", \"r\") as raw_input:\n",
    "        with open(\"/Users/akshay/Downloads/australian_users_items_cleaned.json\", \"w\") as cleaned_file:\n",
    "            try:\n",
    "                for line in raw_input:\n",
    "                    withoutSingleQuotes = line.replace('\\'', '\\\"')\n",
    "                    main_item_name_indexes = [m.start() for m in re.finditer('item_name', withoutSingleQuotes)]\n",
    "                    for main_item_name_index in main_item_name_indexes:\n",
    "                        main_item_name_index = main_item_name_index + 13\n",
    "                        current_play_index = withoutSingleQuotes[main_item_name_index:].find(\"playtime_forever\")\n",
    "                        temp = withoutSingleQuotes[main_item_name_index: main_item_name_index + current_play_index - 4 ]\n",
    "                        repeatingDoubleQuotesIndexes = [m.start() for m in re.finditer(\"\\\"\", temp)]\n",
    "                        for i in repeatingDoubleQuotesIndexes:\n",
    "                            toReplaceIndex = main_item_name_index + i\n",
    "                            withoutSingleQuotes = withoutSingleQuotes[:toReplaceIndex] + \"'\" + withoutSingleQuotes[toReplaceIndex + 1:]\n",
    "                    cleaned_file.write(withoutSingleQuotes)\n",
    "            except:\n",
    "                pass\n",
    "if __name__ == \"__main__\":\n",
    "    clean_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare the dataset\n",
    "\n",
    "- To predict the ratings for users, we need the dataset in the format of <b>user_id,item_id,rating</b> <br>\n",
    "Since the dataset does not have an explicit \"rating\" value, we will use implicit ratings to estimate the user's likeliness towards the game. <br>\n",
    "- Each user entry JSON has a \"playtime_forever\" value. We can use this value to estimate the rating the user would give this game. For example, if the user has been playing a particular game for a long time, we can sufficiently assume the user would rate this game higly. <br>\n",
    "- The \"playtime_forever\" metric values varies a lot between users, so we cannot use the raw value as it would skew our results. For example, if a user's average playtime is 100 hours, and they have spent 5 hours playing a game, then the rating shouldn't be that high. But if a user's average playtime is 10 hours, and they have spent 5 hours playing a gam, then the rating should be high. So we normalize the rating between 1 and 100 by taking into consideration the minimum playtime and maximum playtime of each user.\n",
    "- The playtime is scaled to a rating between 1 and 100 using the following formula:\n",
    "  - <b>scaled_playtime = (1-(playtime-min_playtime)/(max_playtime-min_playtime)) + 100*((playtime-min_playtime)/(max_playtime-min_playtime))</b>\n",
    "- Another decision made is to ignore items if the user has a playtime of 0 for that item. The reason being that there can be several reasons the playtime can be empty. For example, if the user has only recently purchased the game, or the user has not gotten around to playing this game yet. Since we don't have any extra information to infer such reasons, it's better to not include 0 values and it will significantly skew the scaled rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import random\n",
    "\n",
    "def create_dataset():\n",
    "    resultRows = []\n",
    "    with open(\"/Users/akshay/Downloads/australian_users_items_cleaned.json\", \"r\") as cleaned_file:\n",
    "        for line in cleaned_file:\n",
    "            try:\n",
    "                json_obj = json.loads(line)\n",
    "                user_id = str(json_obj['user_id'])\n",
    "                items = json_obj['items']\n",
    "                max_playtime = 0\n",
    "                min_playtime = sys.maxsize\n",
    "                for item in items:\n",
    "                    playtime = item['playtime_forever']\n",
    "                    if playtime > 0:\n",
    "                        if playtime > max_playtime:\n",
    "                            max_playtime = playtime\n",
    "                        if playtime < min_playtime:\n",
    "                            min_playtime = playtime\n",
    "                for item in items:\n",
    "                    item_id = str(item['item_id'])\n",
    "                    playtime = item['playtime_forever']\n",
    "                    if playtime > 0:\n",
    "                        scaled_playtime = (1-(playtime-min_playtime)/(max_playtime-min_playtime)) + 100*((playtime-min_playtime)/(max_playtime-min_playtime))\n",
    "                        resultRows.append([user_id,item_id,scaled_playtime])\n",
    "            except:\n",
    "                pass\n",
    "    random.shuffle(resultRows)\n",
    "    with open(\"/Users/akshay/Downloads/experiment_1_dataset.csv\", \"w\") as experiment_dataset:\n",
    "        experiment_dataset.write(\"user_id,item_id,rating\\n\")\n",
    "        for entry in resultRows:\n",
    "            experiment_dataset.write(entry[0] + \",\" + entry[1] + \",\" + str(entry[2]) + \"\\n\")\n",
    "if __name__ == \"__main__\":\n",
    "    create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataset for analysis looks like this: <br>\n",
    "It has 2601684 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lulllabi</td>\n",
       "      <td>220860</td>\n",
       "      <td>1.154185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76561198076749377</td>\n",
       "      <td>312280</td>\n",
       "      <td>1.490666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76561198040566460</td>\n",
       "      <td>341720</td>\n",
       "      <td>1.613636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cyan101</td>\n",
       "      <td>346900</td>\n",
       "      <td>1.416118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MRmeohme</td>\n",
       "      <td>57300</td>\n",
       "      <td>2.038601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id  item_id    rating\n",
       "0           Lulllabi   220860  1.154185\n",
       "1  76561198076749377   312280  1.490666\n",
       "2  76561198040566460   341720  1.613636\n",
       "3            cyan101   346900  1.416118\n",
       "4           MRmeohme    57300  2.038601"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_full = pd.read_csv('/Users/akshay/Downloads/experiment_1_dataset.csv')\n",
    "users_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3252104, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Training the Model\n",
    "\n",
    "- To perform model training, first we need to split the dataset generated in the previous step into training and testing subsets. As the dataset has 3252104 rows, roughly 80%, i.e 2601685 rows will be part of the training subset, and the rest as testing subset\n",
    "\n",
    "  - Split the experiment_1_dataset.csv using the following command on the terminal:\n",
    "    - split -l 2601685 experiment_1_dataset.csv train\n",
    "    - Rename the traina and trainb files as experiment_1_dataset_train.csv and experiment_1_dataset_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the training dataset as DataFrame and converting to Dataset object for surprise library. \n",
    "# Mention the rating scale 1 - 100 while initilializing the Reader\n",
    "\n",
    "users = pd.read_csv('/Users/akshay/Downloads/experiment_1_dataset_train.csv')\n",
    "reader = Reader(rating_scale=(1, 100))\n",
    "data = Dataset.load_from_df(users[['user_id', 'item_id', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=.20, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will use SVD algorithm to extract information from latent factors. The only information in the dataset about the relationship between games and users is whether the user has purchased the game and the time played. This is not enough information to build models using decision trees or KNN. <br>\n",
    "We perform a Grid Search to find the most suitable hyperparameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   27.2s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   54.3s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of 108 | elapsed: 16.3min remaining:  2.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.118828593579039\n",
      "{'n_factors': 0, 'n_epochs': 60, 'lr_all': 0.0005, 'reg_all': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed: 19.3min finished\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"n_factors\": [0,1,2],\n",
    "    \"n_epochs\": [10, 50, 60],\n",
    "    \"lr_all\": [0.0005, 0.0007],\n",
    "    \"reg_all\": [0.01, 0.09]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(SVD, param_grid, n_jobs=-1, measures=[\"rmse\"], cv=3, joblib_verbose=10)\n",
    "gs.fit(data)\n",
    "\n",
    "print(gs.best_score[\"rmse\"])\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- We run the Grid Search again with different set of parameters. Note the \"-1\" passed for n_jobs paramters. This instructs the Python runtime to utilize all the cores of the machine and run concurrent workers for doing the Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "/usr/local/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed: 16.7min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed: 42.6min\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed: 46.5min\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of 108 | elapsed: 52.0min remaining:  6.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.682724470001697\n",
      "{'n_factors': 9, 'n_epochs': 60, 'lr_all': 0.0001, 'reg_all': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed: 57.2min finished\n"
     ]
    }
   ],
   "source": [
    "param_grid_2 = {\n",
    "    \"n_factors\": [5,7,9],\n",
    "    \"n_epochs\": [60,70,90],\n",
    "    \"lr_all\": [0.0001, 0.0006],\n",
    "    \"reg_all\": [0.12, 0.15]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(SVD, param_grid_2, n_jobs=-1, measures=[\"rmse\"], cv=3, joblib_verbose=10)\n",
    "gs.fit(data)\n",
    "\n",
    "print(gs.best_score[\"rmse\"])\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our earlier Grid Search hyperparamters had a lower RMSE, so we will train the model on the full trainset using those hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Processing epoch 20\n",
      "Processing epoch 21\n",
      "Processing epoch 22\n",
      "Processing epoch 23\n",
      "Processing epoch 24\n",
      "Processing epoch 25\n",
      "Processing epoch 26\n",
      "Processing epoch 27\n",
      "Processing epoch 28\n",
      "Processing epoch 29\n",
      "Processing epoch 30\n",
      "Processing epoch 31\n",
      "Processing epoch 32\n",
      "Processing epoch 33\n",
      "Processing epoch 34\n",
      "Processing epoch 35\n",
      "Processing epoch 36\n",
      "Processing epoch 37\n",
      "Processing epoch 38\n",
      "Processing epoch 39\n",
      "Processing epoch 40\n",
      "Processing epoch 41\n",
      "Processing epoch 42\n",
      "Processing epoch 43\n",
      "Processing epoch 44\n",
      "Processing epoch 45\n",
      "Processing epoch 46\n",
      "Processing epoch 47\n",
      "Processing epoch 48\n",
      "Processing epoch 49\n",
      "Processing epoch 50\n",
      "Processing epoch 51\n",
      "Processing epoch 52\n",
      "Processing epoch 53\n",
      "Processing epoch 54\n",
      "Processing epoch 55\n",
      "Processing epoch 56\n",
      "Processing epoch 57\n",
      "Processing epoch 58\n",
      "Processing epoch 59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x11a0435e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model = SVD(n_factors=0, n_epochs=60, lr_all=0.0005, reg_all=0.09, verbose=True)\n",
    "full_trainset = data.build_full_trainset()\n",
    "svd_model.fit(full_trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform K-Fold Cross Validation to mesaure the accuracy of the model on randomized K=5 splits of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Processing epoch 20\n",
      "Processing epoch 21\n",
      "Processing epoch 22\n",
      "Processing epoch 23\n",
      "Processing epoch 24\n",
      "Processing epoch 25\n",
      "Processing epoch 26\n",
      "Processing epoch 27\n",
      "Processing epoch 28\n",
      "Processing epoch 29\n",
      "Processing epoch 30\n",
      "Processing epoch 31\n",
      "Processing epoch 32\n",
      "Processing epoch 33\n",
      "Processing epoch 34\n",
      "Processing epoch 35\n",
      "Processing epoch 36\n",
      "Processing epoch 37\n",
      "Processing epoch 38\n",
      "Processing epoch 39\n",
      "Processing epoch 40\n",
      "Processing epoch 41\n",
      "Processing epoch 42\n",
      "Processing epoch 43\n",
      "Processing epoch 44\n",
      "Processing epoch 45\n",
      "Processing epoch 46\n",
      "Processing epoch 47\n",
      "Processing epoch 48\n",
      "Processing epoch 49\n",
      "Processing epoch 50\n",
      "Processing epoch 51\n",
      "Processing epoch 52\n",
      "Processing epoch 53\n",
      "Processing epoch 54\n",
      "Processing epoch 55\n",
      "Processing epoch 56\n",
      "Processing epoch 57\n",
      "Processing epoch 58\n",
      "Processing epoch 59\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Processing epoch 20\n",
      "Processing epoch 21\n",
      "Processing epoch 22\n",
      "Processing epoch 23\n",
      "Processing epoch 24\n",
      "Processing epoch 25\n",
      "Processing epoch 26\n",
      "Processing epoch 27\n",
      "Processing epoch 28\n",
      "Processing epoch 29\n",
      "Processing epoch 30\n",
      "Processing epoch 31\n",
      "Processing epoch 32\n",
      "Processing epoch 33\n",
      "Processing epoch 34\n",
      "Processing epoch 35\n",
      "Processing epoch 36\n",
      "Processing epoch 37\n",
      "Processing epoch 38\n",
      "Processing epoch 39\n",
      "Processing epoch 40\n",
      "Processing epoch 41\n",
      "Processing epoch 42\n",
      "Processing epoch 43\n",
      "Processing epoch 44\n",
      "Processing epoch 45\n",
      "Processing epoch 46\n",
      "Processing epoch 47\n",
      "Processing epoch 48\n",
      "Processing epoch 49\n",
      "Processing epoch 50\n",
      "Processing epoch 51\n",
      "Processing epoch 52\n",
      "Processing epoch 53\n",
      "Processing epoch 54\n",
      "Processing epoch 55\n",
      "Processing epoch 56\n",
      "Processing epoch 57\n",
      "Processing epoch 58\n",
      "Processing epoch 59\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Processing epoch 20\n",
      "Processing epoch 21\n",
      "Processing epoch 22\n",
      "Processing epoch 23\n",
      "Processing epoch 24\n",
      "Processing epoch 25\n",
      "Processing epoch 26\n",
      "Processing epoch 27\n",
      "Processing epoch 28\n",
      "Processing epoch 29\n",
      "Processing epoch 30\n",
      "Processing epoch 31\n",
      "Processing epoch 32\n",
      "Processing epoch 33\n",
      "Processing epoch 34\n",
      "Processing epoch 35\n",
      "Processing epoch 36\n",
      "Processing epoch 37\n",
      "Processing epoch 38\n",
      "Processing epoch 39\n",
      "Processing epoch 40\n",
      "Processing epoch 41\n",
      "Processing epoch 42\n",
      "Processing epoch 43\n",
      "Processing epoch 44\n",
      "Processing epoch 45\n",
      "Processing epoch 46\n",
      "Processing epoch 47\n",
      "Processing epoch 48\n",
      "Processing epoch 49\n",
      "Processing epoch 50\n",
      "Processing epoch 51\n",
      "Processing epoch 52\n",
      "Processing epoch 53\n",
      "Processing epoch 54\n",
      "Processing epoch 55\n",
      "Processing epoch 56\n",
      "Processing epoch 57\n",
      "Processing epoch 58\n",
      "Processing epoch 59\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Processing epoch 20\n",
      "Processing epoch 21\n",
      "Processing epoch 22\n",
      "Processing epoch 23\n",
      "Processing epoch 24\n",
      "Processing epoch 25\n",
      "Processing epoch 26\n",
      "Processing epoch 27\n",
      "Processing epoch 28\n",
      "Processing epoch 29\n",
      "Processing epoch 30\n",
      "Processing epoch 31\n",
      "Processing epoch 32\n",
      "Processing epoch 33\n",
      "Processing epoch 34\n",
      "Processing epoch 35\n",
      "Processing epoch 36\n",
      "Processing epoch 37\n",
      "Processing epoch 38\n",
      "Processing epoch 39\n",
      "Processing epoch 40\n",
      "Processing epoch 41\n",
      "Processing epoch 42\n",
      "Processing epoch 43\n",
      "Processing epoch 44\n",
      "Processing epoch 45\n",
      "Processing epoch 46\n",
      "Processing epoch 47\n",
      "Processing epoch 48\n",
      "Processing epoch 49\n",
      "Processing epoch 50\n",
      "Processing epoch 51\n",
      "Processing epoch 52\n",
      "Processing epoch 53\n",
      "Processing epoch 54\n",
      "Processing epoch 55\n",
      "Processing epoch 56\n",
      "Processing epoch 57\n",
      "Processing epoch 58\n",
      "Processing epoch 59\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Processing epoch 20\n",
      "Processing epoch 21\n",
      "Processing epoch 22\n",
      "Processing epoch 23\n",
      "Processing epoch 24\n",
      "Processing epoch 25\n",
      "Processing epoch 26\n",
      "Processing epoch 27\n",
      "Processing epoch 28\n",
      "Processing epoch 29\n",
      "Processing epoch 30\n",
      "Processing epoch 31\n",
      "Processing epoch 32\n",
      "Processing epoch 33\n",
      "Processing epoch 34\n",
      "Processing epoch 35\n",
      "Processing epoch 36\n",
      "Processing epoch 37\n",
      "Processing epoch 38\n",
      "Processing epoch 39\n",
      "Processing epoch 40\n",
      "Processing epoch 41\n",
      "Processing epoch 42\n",
      "Processing epoch 43\n",
      "Processing epoch 44\n",
      "Processing epoch 45\n",
      "Processing epoch 46\n",
      "Processing epoch 47\n",
      "Processing epoch 48\n",
      "Processing epoch 49\n",
      "Processing epoch 50\n",
      "Processing epoch 51\n",
      "Processing epoch 52\n",
      "Processing epoch 53\n",
      "Processing epoch 54\n",
      "Processing epoch 55\n",
      "Processing epoch 56\n",
      "Processing epoch 57\n",
      "Processing epoch 58\n",
      "Processing epoch 59\n",
      "Evaluating RMSE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    15.1230 15.1291 15.1131 15.1184 15.1406 15.1248 0.0095  \n",
      "Fit time          92.75   95.74   93.72   95.69   91.75   93.93   1.59    \n",
      "Test time         8.09    7.97    8.37    7.32    7.36    7.82    0.41    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([15.12304094, 15.12905844, 15.11311565, 15.1183519 , 15.14055615]),\n",
       " 'fit_time': (92.7524209022522,\n",
       "  95.74028491973877,\n",
       "  93.7231879234314,\n",
       "  95.69433236122131,\n",
       "  91.7487223148346),\n",
       " 'test_time': (8.085649967193604,\n",
       "  7.969265937805176,\n",
       "  8.369022846221924,\n",
       "  7.3205320835113525,\n",
       "  7.356119871139526)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(svd_model, data, measures=['RMSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Make Predictions using the SVD Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that the model is trained, we can make predictions of a user rating for a particular game (item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.283107543017255\n"
     ]
    }
   ],
   "source": [
    "user_id = \"deathfatel\"\n",
    "item_id = 377160\n",
    "pred = svd_model.predict(user_id, item_id)\n",
    "print(pred.est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Generate dataset for the Recommendation classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can now use this ratings predictor model to recommend games as well as game bundles to user. For performing that task, we need to be able to predict whether to recommend a particular game to a user or not. We need a new prepared dataset to perform this hypothesis. The dataset should have the following schema:\n",
    "  - <b>user_id,item_id,class</b>\n",
    "  \n",
    "- This dataset can be generated from experiment_1_dataset_test.csv. If the predicted rating is above 50, we recommend the game to the user\n",
    "- For each row, if the predicted rating is above 50, set the class as True, else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_data():\n",
    "    with open(\"/Users/akshay/Downloads/experiment_1_dataset_test.csv\", \"r\") as test_file:\n",
    "        with open(\"/Users/akshay/Downloads/experiment_1_dataset_test_with_class.csv\", \"w\") as class_test_file:\n",
    "            for line in test_file:\n",
    "                splits = line.split(\",\")\n",
    "                user_id, item_id, rating = splits[0], splits[1], splits[2]\n",
    "                if float(rating) > 50:\n",
    "                    class_test_file.write(user_id + \",\" + item_id + \",\" + \"True\\n\")\n",
    "                else:\n",
    "                    class_test_file.write(user_id + \",\" + item_id + \",\" + \"False\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate the Recommendation classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use the generated model to predict the user rating for an item, and subsequently the class. If the predicted class and actual class is the same, we have made a successful recommendation. \n",
    "- Based on the above technique of evaluation, we calculate the True Positive, True Negative, False Positive and False Negative metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive:  4324\n",
      "True Negative:  620739\n",
      "False Positive:  4416\n",
      "False Negative:  20941\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "resultRows = []\n",
    "with open('/Users/akshay/Downloads/experiment_1_dataset_test_with_class.csv') as csvfile:\n",
    "    fail = 0\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        try:\n",
    "            user_id = row[0]\n",
    "            item_id = int(row[1])\n",
    "            actual_class = row[2]\n",
    "            if actual_class == \"False\":\n",
    "                actual_class = False\n",
    "            else:\n",
    "                actual_class = True\n",
    "            pred = svd_model.predict(user_id, item_id)\n",
    "            predicted_rating = float(pred.est)\n",
    "            if predicted_rating > 50.0:\n",
    "                predicted_class = True\n",
    "            else:\n",
    "                predicted_class = False\n",
    "            if predicted_class == True and actual_class == True:\n",
    "                #print(user_id, item_id, predicted_rating, predicted_class, actual_class)\n",
    "                true_positive = true_positive + 1\n",
    "            elif predicted_class == False and actual_class == False:\n",
    "                true_negative = true_negative + 1\n",
    "            elif predicted_class == True and actual_class == False:\n",
    "                false_positive = false_positive + 1\n",
    "            elif predicted_class == False and actual_class == True:\n",
    "                false_negative = false_negative + 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "       \n",
    "print(\"True Positive: \", true_positive)\n",
    "print(\"True Negative: \", true_negative)\n",
    "print(\"False Positive: \", false_positive)\n",
    "print(\"False Negative: \", false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49473684210526314\n",
      "0.1711458539481496\n"
     ]
    }
   ],
   "source": [
    "precision = true_positive/(true_positive + false_positive)\n",
    "recall = true_positive/(true_positive + false_negative)\n",
    "\n",
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Get Recommendation for a user_id for any item_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can get recommendation values for each user_id - item_id combination as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_recommend(user_id, item_id):\n",
    "    pred = svd_model.predict(user_id, item_id)\n",
    "    predicted_rating = float(pred.est)\n",
    "    print(predicted_rating)\n",
    "    if predicted_rating > 50.0:\n",
    "        predicted_class = True\n",
    "    else:\n",
    "        predicted_class = False\n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.65173220261376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_recommend(\"RhinoSquad\",377160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Make bundle predictions for users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now for each user, we can calculate the cumulative rating of the entire game bundle. This function, given a user_id, will calculate the rating for each item in each game bundle. If the cumulative rating of the bundle is above a threshold, the whole bundle is recommended to the user\n",
    "- The threshold selected for this analysis is 25, i.e if the cumulative rating for the bundle by the user is above 25, then the bundle is recommended to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: nosferatuzodd  Bundles: ['The Binding of Isaac : Rebirth + Afterbirth Bundle', 'Left 4 Dead Bundle']\n",
      "\n",
      "User: 76561198043626578  Bundles: ['The Binding of Isaac : Rebirth + Afterbirth Bundle', 'Left 4 Dead Bundle']\n",
      "\n",
      "User: 76561198033718152  Bundles: ['The Elder Scrolls V: Skyrim + Add-Ons', 'The Binding of Isaac : Rebirth + Afterbirth Bundle', 'Left 4 Dead Bundle']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_bundles_for_all_users():\n",
    "    user_ids = []\n",
    "    with open(\"/Users/akshay/Downloads/experiment_1_dataset_test.csv\", \"r\") as users_file:\n",
    "        readCSV = csv.reader(users_file, delimiter=',')\n",
    "        for row in readCSV:\n",
    "            user_id = row[0]\n",
    "            with open(\"/Users/akshay/Downloads/bundle_data_cleaned.json\", \"r\") as cleaned_file:\n",
    "                bundle_reccos = []\n",
    "                for line in cleaned_file:\n",
    "                    try:\n",
    "                        bundle_rating = 0\n",
    "                        json_obj = json.loads(line)\n",
    "                        items = json_obj[\"items\"]\n",
    "                        bundle_name = json_obj[\"bundle_name\"]\n",
    "                        for item in items:\n",
    "                            item_id = int(item[\"item_id\"])\n",
    "                            pred = svd_model.predict(user_id, item_id)\n",
    "                            predicted_rating = float(pred.est)\n",
    "                            bundle_rating = bundle_rating + predicted_rating\n",
    "                            \n",
    "                        average_rating_for_bundle = float(bundle_rating/len(items))\n",
    "                        if average_rating_for_bundle > 25.0:\n",
    "                            bundle_reccos.append(bundle_name)\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                if len(bundle_reccos) > 1 and len(bundle_reccos) < 10:\n",
    "                    print(\"User: \" + user_id + \"  Bundles: \" + str(bundle_reccos) + \"\\n\")\n",
    "\n",
    "get_bundles_for_all_users()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To generate bundle recommendation for a specific user, the following code snippet can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 76561198033718152  Bundles: ['The Elder Scrolls V: Skyrim + Add-Ons', 'The Binding of Isaac : Rebirth + Afterbirth Bundle', 'Left 4 Dead Bundle']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_bundles_for_user(user_id):\n",
    "    with open(\"/Users/akshay/Downloads/bundle_data_cleaned.json\", \"r\") as cleaned_file:\n",
    "        bundle_reccos = []\n",
    "        for line in cleaned_file:\n",
    "            try:\n",
    "                bundle_rating = 0\n",
    "                json_obj = json.loads(line)\n",
    "                items = json_obj[\"items\"]\n",
    "                bundle_name = json_obj[\"bundle_name\"]\n",
    "                for item in items:\n",
    "                    item_id = int(item[\"item_id\"])\n",
    "                    pred = svd_model.predict(user_id, item_id)\n",
    "                    predicted_rating = float(pred.est)\n",
    "                    bundle_rating = bundle_rating + predicted_rating\n",
    "\n",
    "                average_rating_for_bundle = float(bundle_rating/len(items))\n",
    "                if average_rating_for_bundle > 25.0:\n",
    "                    bundle_reccos.append(bundle_name)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        if len(bundle_reccos) > 1 and len(bundle_reccos) < 10:\n",
    "            print(\"User: \" + user_id + \"  Bundles: \" + str(bundle_reccos) + \"\\n\")\n",
    "\n",
    "get_bundles_for_user(\"76561198033718152\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
