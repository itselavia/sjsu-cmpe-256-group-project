{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering usinf Spark(Pyspark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: \n",
    "* Import the pyspark libary and create spark session\n",
    "* Import other required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "import requests, json, os, sys, time, re\n",
    "from sklearn.metrics.pairwise import linear_kernel,cosine_similarity\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Spark session is a unified entry point of a spark application  #############\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('spark-ALS') \\\n",
    "    .config(\"configuration_key\", \"configuration_value\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import itertools\n",
    "from math import sqrt\n",
    "from operator import add\n",
    "from os.path import join, isfile, dirname\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark.sql.types import TimestampType\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : \n",
    "* Load the already preprocessed data into sparkRDD\n",
    "* Reformat the data as per need of Spark MLib\n",
    "* analyzing the loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_raw_data = sc.textFile(\"../../pre_processed_data/experiment_1_dataset.csv\")\n",
    "ratings_raw_data_header = ratings_raw_data.take(1)[0]\n",
    "ratings_data = ratings_raw_data.filter(lambda line: line != ratings_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(hashlib.sha1(tokens[0].encode('utf-8')).hexdigest(), 16) % (10 ** 8),tokens[1],int(float(tokens[2][:10])))).cache()\n",
    "\n",
    "#ratings_data = ratings_data.map(lambda x: (x[0] , x[1], int(int(x[2])/10) +1.0 ))\n",
    "#ratings_data = ratings_data.filter(lambda x: int(x[2]) >=2 )\n",
    "rddTraining, rddValidating, rddTesting = ratings_data.randomSplit([6,2,2], seed=1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(64592103, '55150', 1),\n",
       " (46027953, '12900', 1),\n",
       " (11418539, '4000', 100),\n",
       " (22844952, '363970', 10),\n",
       " (30902696, '317360', 4),\n",
       " (29257067, '65800', 1),\n",
       " (68176534, '218230', 6),\n",
       " (35300949, '240', 5),\n",
       " (5765927, '65700', 1),\n",
       " (39525566, '221100', 7)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: long (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      " |-- rating: long (nullable = true)\n",
      "\n",
      "+--------+------+------+\n",
      "|user    |item  |rating|\n",
      "+--------+------+------+\n",
      "|64592103|55150 |1     |\n",
      "|46027953|12900 |1     |\n",
      "|11418539|4000  |100   |\n",
      "|22844952|363970|10    |\n",
      "|30902696|317360|4     |\n",
      "|29257067|65800 |1     |\n",
      "|68176534|218230|6     |\n",
      "|35300949|240   |5     |\n",
      "|5765927 |65700 |1     |\n",
      "|39525566|221100|7     |\n",
      "|59586993|291410|2     |\n",
      "|20441736|304930|1     |\n",
      "|8742321 |4000  |8     |\n",
      "|62551528|113400|4     |\n",
      "|67019253|238460|1     |\n",
      "|49237568|242860|3     |\n",
      "|58504375|235800|2     |\n",
      "|2591283 |10150 |9     |\n",
      "|90966574|280790|2     |\n",
      "|96131366|221040|2     |\n",
      "+--------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rddTraining.take(10)\n",
    "#new_ratings_data = ratings_data.filter(lambda x: int(x[2]) >=2)\n",
    "schema = [\"user\",\"item\",\"rating\"]\n",
    "df = spark.createDataFrame(data=ratings_data, schema = schema)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|rating|  count|\n",
      "+------+-------+\n",
      "|    26|   7493|\n",
      "|    29|   6215|\n",
      "|    65|   1458|\n",
      "|    19|  13032|\n",
      "|    54|   2056|\n",
      "|    22|   9863|\n",
      "|     7|  61709|\n",
      "|    77|   1059|\n",
      "|    34|   4713|\n",
      "|    50|   2288|\n",
      "|    94|    726|\n",
      "|    57|   1819|\n",
      "|    43|   3200|\n",
      "|    32|   5239|\n",
      "|    84|    874|\n",
      "|    31|   5659|\n",
      "|    39|   3632|\n",
      "|    98|    636|\n",
      "|    25|   7995|\n",
      "|    95|    714|\n",
      "|    71|   1270|\n",
      "|     6|  77079|\n",
      "|    68|   1278|\n",
      "|    72|   1220|\n",
      "|    87|    832|\n",
      "|    58|   1783|\n",
      "|     9|  41852|\n",
      "|    27|   7129|\n",
      "|    63|   1538|\n",
      "|    56|   1951|\n",
      "|    51|   2260|\n",
      "|    52|   2131|\n",
      "|    17|  15388|\n",
      "|    79|    989|\n",
      "|    41|   3331|\n",
      "|    33|   4906|\n",
      "|    28|   6711|\n",
      "|    88|    829|\n",
      "|     5| 101001|\n",
      "|     1|1687641|\n",
      "|    96|    719|\n",
      "|    10|  36035|\n",
      "|    89|    806|\n",
      "|    85|    880|\n",
      "|    67|   1360|\n",
      "|    48|   2546|\n",
      "|   100|  65991|\n",
      "|    44|   2897|\n",
      "|    61|   1567|\n",
      "|     3| 209694|\n",
      "|    37|   4044|\n",
      "|    83|    928|\n",
      "|    12|  27145|\n",
      "|    55|   1960|\n",
      "|    74|   1132|\n",
      "|     8|  50625|\n",
      "|    62|   1519|\n",
      "|    11|  30693|\n",
      "|    49|   2457|\n",
      "|    35|   4473|\n",
      "|    80|   1026|\n",
      "|     2| 377532|\n",
      "|    66|   1377|\n",
      "|    76|   1115|\n",
      "|     4| 138818|\n",
      "|    92|    756|\n",
      "|    13|  23457|\n",
      "|    75|   1081|\n",
      "|    36|   4319|\n",
      "|    78|   1015|\n",
      "|    18|  13870|\n",
      "|    69|   1280|\n",
      "|    14|  21240|\n",
      "|    21|  10705|\n",
      "|    59|   1708|\n",
      "|    81|    946|\n",
      "|    15|  18969|\n",
      "|    82|    943|\n",
      "|    38|   3848|\n",
      "|    97|    673|\n",
      "|    30|   5784|\n",
      "|    42|   3215|\n",
      "|    73|   1149|\n",
      "|    90|    819|\n",
      "|    23|   9264|\n",
      "|    46|   2760|\n",
      "|    20|  11833|\n",
      "|    70|   1271|\n",
      "|    99|    618|\n",
      "|    86|    831|\n",
      "|    60|   1638|\n",
      "|    93|    713|\n",
      "|    40|   3491|\n",
      "|    16|  16988|\n",
      "|    64|   1391|\n",
      "|    91|    764|\n",
      "|    53|   2131|\n",
      "|    45|   2775|\n",
      "|    47|   2637|\n",
      "|    24|   8384|\n",
      "+------+-------+\n",
      "\n",
      "+--------+------+\n",
      "|count(1)|rating|\n",
      "+--------+------+\n",
      "| 1687641|     1|\n",
      "|  377532|     2|\n",
      "|  209694|     3|\n",
      "|  138818|     4|\n",
      "|  101001|     5|\n",
      "|   77079|     6|\n",
      "|   61709|     7|\n",
      "|   50625|     8|\n",
      "|   41852|     9|\n",
      "|   36035|    10|\n",
      "|   30693|    11|\n",
      "|   27145|    12|\n",
      "|   23457|    13|\n",
      "|   21240|    14|\n",
      "|   18969|    15|\n",
      "|   16988|    16|\n",
      "|   15388|    17|\n",
      "|   13870|    18|\n",
      "|   13032|    19|\n",
      "|   11833|    20|\n",
      "|   10705|    21|\n",
      "|    9863|    22|\n",
      "|    9264|    23|\n",
      "|    8384|    24|\n",
      "|    7995|    25|\n",
      "|    7493|    26|\n",
      "|    7129|    27|\n",
      "|    6711|    28|\n",
      "|    6215|    29|\n",
      "|    5784|    30|\n",
      "|    5659|    31|\n",
      "|    5239|    32|\n",
      "|    4906|    33|\n",
      "|    4713|    34|\n",
      "|    4473|    35|\n",
      "|    4319|    36|\n",
      "|    4044|    37|\n",
      "|    3848|    38|\n",
      "|    3632|    39|\n",
      "|    3491|    40|\n",
      "|    3331|    41|\n",
      "|    3215|    42|\n",
      "|    3200|    43|\n",
      "|    2897|    44|\n",
      "|    2775|    45|\n",
      "|    2760|    46|\n",
      "|    2637|    47|\n",
      "|    2546|    48|\n",
      "|    2457|    49|\n",
      "|    2288|    50|\n",
      "|    2260|    51|\n",
      "|    2131|    52|\n",
      "|    2131|    53|\n",
      "|    2056|    54|\n",
      "|    1960|    55|\n",
      "|    1951|    56|\n",
      "|    1819|    57|\n",
      "|    1783|    58|\n",
      "|    1708|    59|\n",
      "|    1638|    60|\n",
      "|    1567|    61|\n",
      "|    1519|    62|\n",
      "|    1538|    63|\n",
      "|    1391|    64|\n",
      "|    1458|    65|\n",
      "|    1377|    66|\n",
      "|    1360|    67|\n",
      "|    1278|    68|\n",
      "|    1280|    69|\n",
      "|    1271|    70|\n",
      "|    1270|    71|\n",
      "|    1220|    72|\n",
      "|    1149|    73|\n",
      "|    1132|    74|\n",
      "|    1081|    75|\n",
      "|    1115|    76|\n",
      "|    1059|    77|\n",
      "|    1015|    78|\n",
      "|     989|    79|\n",
      "|    1026|    80|\n",
      "|     946|    81|\n",
      "|     943|    82|\n",
      "|     928|    83|\n",
      "|     874|    84|\n",
      "|     880|    85|\n",
      "|     831|    86|\n",
      "|     832|    87|\n",
      "|     829|    88|\n",
      "|     806|    89|\n",
      "|     819|    90|\n",
      "|     764|    91|\n",
      "|     756|    92|\n",
      "|     713|    93|\n",
      "|     726|    94|\n",
      "|     714|    95|\n",
      "|     719|    96|\n",
      "|     673|    97|\n",
      "|     636|    98|\n",
      "|     618|    99|\n",
      "|   65991|   100|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"rating\").count().show(100)\n",
    "df.registerTempTable(\"df\");\n",
    "spark.sql(\"select count(*) , rating from df group by rating order by rating\").show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Finding the best parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/Akxay/recommendation_engine/blob/master/Jobs_RE_spark.ipynb\n",
    "#Using function to compare the best possible parameters\n",
    "def howFarAreWe(model, against, sizeAgainst):\n",
    "    againstNoRatings = against.map(lambda x: (int(x[0]), int(x[1])) )\n",
    "    againstWiRatings = against.map(lambda x: ((int(x[0]),int(x[1])), int(x[2])) )\n",
    "    predictions = model.predictAll(againstNoRatings).map(lambda p: ( (p[0],p[1]), p[2]) )\n",
    "    predictionsAndRatings = predictions.join(againstWiRatings).values()    \n",
    "    return sqrt(predictionsAndRatings.map(lambda s: (s[0] - s[1]) ** 2).reduce(add) / float(sizeAgainst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5 0.01\n",
      "Best so far:18.991656\n",
      "5 5 0.02\n",
      "Best so far:18.985128\n",
      "10 5 0.02\n",
      "Best so far:18.982636\n",
      "Rank 5\n",
      "Iter 10\n",
      "Dist 18.982636\n",
      "Alpha 0.020000\n"
     ]
    }
   ],
   "source": [
    "#finding best set of parameters\n",
    "ranks  = [5,10]\n",
    "iters  = [5,10]\n",
    "alpha = [0.01, 0.02]\n",
    "\n",
    "finalModel = None\n",
    "finalRank  = 0\n",
    "finalIter  = -1\n",
    "finalDist   = float(300)\n",
    "finalAlpha = float(0)\n",
    "\n",
    "#[START train_model]\n",
    "for cRank, cIter, cAlpha in itertools.product(ranks, iters, alpha):\n",
    "    model = ALS.trainImplicit(rddTraining, cRank, cIter, alpha=float(cAlpha))\n",
    "    dist = howFarAreWe(model, rddValidating, rddValidating.count())\n",
    "    if dist < finalDist:\n",
    "        print(cIter, cRank,cAlpha)\n",
    "        print(\"Best so far:%f\" % dist)\n",
    "        finalModel = model\n",
    "        finalRank  = cRank\n",
    "        finalIter  = cIter\n",
    "        finalDist  = dist\n",
    "        finalAlpha  = cAlpha \n",
    "#[END train_model]\n",
    "\n",
    "print(\"Rank %i\" % finalRank)  \n",
    "print(\"Iter %i\" % finalIter)  \n",
    "print(\"Dist %f\" % finalDist) \n",
    "print(\"Alpha %f\" % finalAlpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training the best parameters and validating the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 10   \n",
    "# The size of the feature vector used; the minimum value is 10, the minimum value of the feature vector, \n",
    "#the better the model produced, but it also costs more calculation cost\n",
    "numIterations = 10\n",
    "#Iteration numbers\n",
    "alpha=0.01\n",
    "#model = ALS.trainImplicit(rddTraining, 10, 10,alpha=0.01) #18.99407447555247\n",
    "#model = ALS.trainImplicit(rddTraining, 10, 10,alpha=0.5) #18.88809845032351\n",
    "#model = ALS.train(rddTraining, 10, 10) #29.13044908080498\n",
    "model = ALS.trainImplicit(rddTraining, rank, numIterations, alpha=0.01) #18.96886550953952\n",
    "testset = sc.parallelize([(80937808, 386360), (80937808, 380600)])   \n",
    "model.predictAll(testset).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((80937808, 386360), 0.16070414091136048),\n",
       " ((80937808, 380600), 0.048304704863768),\n",
       " ((80937808, 315640), 0.0799210385878367),\n",
       " ((80937808, 226320), 0.11713325371411347),\n",
       " ((80937808, 339610), 0.09109506708076365)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate all predictions\n",
    "rddTesting_map = rddTesting.map(lambda r: ((r[0], r[1]))) \n",
    "predictions = model.predictAll(rddTesting_map).map(lambda r: ((r[0], r[1]), (r[2]))) \n",
    "predictions.take(5)    ####### Output 5 results\n",
    "#model.predictAll(rddTesting_map).collect()     Show all the Recommendation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates_and_preds = rddTesting.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions) \n",
    "#rates_and_preds.filter(lambda x : x[1][0] >= 10).take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Calculate the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 18.99229791297807\n"
     ]
    }
   ],
   "source": [
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()) \n",
    "print ('For testing data the RMSE is %s' % (error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Conculsion\n",
    "\n",
    "RMSE score of around 19 is good for the datasets considering the sparse data set. The rating are on par with SVD model but this program executes 3 times faster than surprise SVD due to high parallel processing done by Apache Spark.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Referenece: https://towardsdatascience.com/steam-recommendation-systems-4358917288eb\n",
    "#https://github.com/Akxay/recommendation_engine/blob/master/Jobs_RE_spark.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
